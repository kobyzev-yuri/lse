"""
–ú–æ–¥—É–ª—å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ RSS —Ñ–∏–¥–æ–≤ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã—Ö –±–∞–Ω–∫–æ–≤
"""

import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø—Ä–æ–µ–∫—Ç–∞ –≤ –ø—É—Ç—å
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import feedparser
import logging
import re
import requests
from datetime import datetime
from typing import List, Dict, Optional
from sqlalchemy import create_engine, text

from config_loader import get_database_url

logger = logging.getLogger(__name__)


# RSS —Ñ–∏–¥—ã —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã—Ö –±–∞–Ω–∫–æ–≤
RSS_FEEDS = {
    'FOMC_STATEMENT': {
        'url': 'https://www.federalreserve.gov/feeds/press_all.xml',
        'region': 'USA',
        'event_type': 'FOMC_STATEMENT',
        'importance': 'HIGH'
    },
    'FOMC_SPEECH': {
        # –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π URL: https://www.federalreserve.gov/feeds/feeds.htm (All Speeches)
        # –ù–µ press_speeches.xml ‚Äî —Ç–æ—Ç URL –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç 404
        'url': 'https://www.federalreserve.gov/feeds/speeches.xml',
        'region': 'USA',
        'event_type': 'FOMC_SPEECH',
        'importance': 'HIGH'
    },
    'FOMC_MONETARY': {
        'url': 'https://www.federalreserve.gov/feeds/press_monetary.xml',
        'region': 'USA',
        'event_type': 'FOMC_STATEMENT',
        'importance': 'HIGH'
    },
    'BOE_STATEMENT': {
        # /rss ‚Äî —ç—Ç–æ HTML-—Å—Ç—Ä–∞–Ω–∏—Ü–∞ —Å–æ —Å–ø–∏—Å–∫–æ–º —Ñ–∏–¥–æ–≤; –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∞–ª—å–Ω—ã–π RSS news
        'url': 'https://www.bankofengland.co.uk/rss/news',
        'region': 'UK',
        'event_type': 'BOE_STATEMENT',
        'importance': 'HIGH'
    },
    'ECB_STATEMENT': {
        'url': 'https://www.ecb.europa.eu/rss/press.html',
        'region': 'EU',
        'event_type': 'ECB_STATEMENT',
        'importance': 'HIGH'
    },
    'BOJ_STATEMENT': {
        'url': 'https://www.boj.or.jp/en/rss/whatsnew.xml',
        'region': 'Japan',
        'event_type': 'BOJ_STATEMENT',
        'importance': 'HIGH'
    }
}


def _sanitize_xml_for_parser(text: str, aggressive: bool = False) -> str:
    """
    –û—á–∏—Å—Ç–∫–∞ XML –¥–ª—è –ø–∞—Ä—Å–µ—Ä–∞. –ï—Å–ª–∏ aggressive=True (–¥–ª—è –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Ñ–∏–¥–æ–≤ –≤—Ä–æ–¥–µ BOE),
    –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –∑–∞–º–µ–Ω—è–µ–º —Å–∏–º–≤–æ–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ —á–∞—Å—Ç–æ –ª–æ–º–∞—é—Ç –ø–∞—Ä—Å–∏–Ω–≥.
    """
    # –£–±–∏—Ä–∞–µ–º –Ω–µ–≤–µ—Ä–Ω—É—é –¥–µ–∫–ª–∞—Ä–∞—Ü–∏—é –∫–æ–¥–∏—Ä–æ–≤–∫–∏
    text = re.sub(r'encoding\s*=\s*["\']us-ascii["\']', 'encoding="utf-8"', text, flags=re.I)
    # –£–±–∏—Ä–∞–µ–º –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–µ –¥–ª—è XML 1.0 —Å–∏–º–≤–æ–ª—ã (control chars + C1 range)
    text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x9f]', '', text)
    # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ ¬´–≥–æ–ª—ã–π¬ª & (–Ω–µ —Ç—Ä–æ–≥–∞–µ–º &amp; &#123; &#x1F; –∏ —Ç.–¥.)
    text = re.sub(r'&(?!(?:#\d+|#x[0-9a-fA-F]+|[a-zA-Z]+);)', '&amp;', text)
    text = re.sub(r'&#0+;', '', text)
    if aggressive:
        # –î–ª—è BOE –∏ –ø–æ–¥–æ–±–Ω—ã—Ö: —Å–∏–º–≤–æ–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ —á–∞—Å—Ç–æ –¥–∞—é—Ç "invalid token"
        text = text.replace('\u2018', "'").replace('\u2019', "'")  # smart quotes
        text = text.replace('\u201c', '"').replace('\u201d', '"')
        text = text.replace('\u2013', '-').replace('\u2014', '-')  # en/em dash
        text = text.replace('\u00a0', ' ')  # nbsp
    return text


def _fetch_rss_content(url: str, aggressive_sanitize: bool = False) -> Optional[str]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç RSS –ø–æ URL —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫–∏ (–º–Ω–æ–≥–∏–µ —Ñ–∏–¥—ã –æ–±—ä—è–≤–ª—è—é—Ç
    us-ascii, –Ω–æ –æ—Ç–¥–∞—é—Ç utf-8 ‚Äî –∏–∑-–∑–∞ —ç—Ç–æ–≥–æ –ø–∞–¥–∞–µ—Ç –ø–∞—Ä—Å–µ—Ä).
    aggressive_sanitize: –¥–ª—è –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Ñ–∏–¥–æ–≤ (BOE) ‚Äî –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞.
    """
    try:
        r = requests.get(url, timeout=30, headers={
            'User-Agent': 'Mozilla/5.0 (compatible; LSE-NewsBot/1.0)',
            'Accept': 'application/rss+xml, application/xml, text/xml',
        })
        r.raise_for_status()
        raw = r.content
        try:
            text = raw.decode('utf-8', errors='replace')
        except Exception:
            text = raw.decode('cp1252', errors='replace')
        text = _sanitize_xml_for_parser(text, aggressive=aggressive_sanitize)
        return text
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {url}: {e}")
        return None


def parse_rss_feed(feed_config: Dict) -> List[Dict]:
    """
    –ü–∞—Ä—Å–∏—Ç RSS —Ñ–∏–¥ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π
    
    Args:
        feed_config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ñ–∏–¥–∞ (url, region, event_type, importance)
        
    Returns:
        –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏
    """
    url = feed_config['url']
    region = feed_config['region']
    event_type = feed_config['event_type']
    importance = feed_config['importance']
    
    try:
        # BOE —Ñ–∏–¥ —á–∞—Å—Ç–æ —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–∏–º–≤–æ–ª—ã, –ª–æ–º–∞—é—â–∏–µ XML ‚Äî –≤–∫–ª—é—á–∞–µ–º –∂—ë—Å—Ç–∫—É—é —Å–∞–Ω–∏—Ç–∏–∑–∞—Ü–∏—é
        aggressive = 'bankofengland' in url.lower()
        content = _fetch_rss_content(url, aggressive_sanitize=aggressive)
        if not content:
            return []
        feed = feedparser.parse(content)
        
        if feed.bozo:
            # –ï—Å–ª–∏ –µ—Å—Ç—å bozo, –Ω–æ –µ—Å—Ç—å entries - –≤—Å–µ —Ä–∞–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö
            if not feed.entries:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ RSS —Ñ–∏–¥–∞ {url}: {feed.bozo_exception}")
                return []
            else:
                logger.warning(f"‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {url}: {feed.bozo_exception}, –Ω–æ –Ω–∞–π–¥–µ–Ω–æ {len(feed.entries)} –∑–∞–ø–∏—Å–µ–π")
        
        items = []
        for entry in feed.entries:
            # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
            published_time = None
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                published_time = datetime(*entry.published_parsed[:6])
            elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                published_time = datetime(*entry.updated_parsed[:6])
            else:
                published_time = datetime.now()
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
            content = entry.summary if hasattr(entry, 'summary') else entry.title
            if hasattr(entry, 'content') and entry.content:
                # –ï—Å–ª–∏ –µ—Å—Ç—å content, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
                if isinstance(entry.content, list) and len(entry.content) > 0:
                    content = entry.content[0].get('value', content)
            
            item = {
                'title': entry.title,
                'link': entry.link if hasattr(entry, 'link') else '',
                'content': content,
                'published': published_time,
                'ticker': 'US_MACRO' if region == 'USA' else 'MACRO',
                'source': f"{region} Central Bank",
                'event_type': event_type,
                'region': region,
                'importance': importance
            }
            items.append(item)
        
        logger.info(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(items)} –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ {event_type}")
        return items
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ RSS —Ñ–∏–¥–∞ {url}: {e}")
        return []


def fetch_all_rss_feeds() -> List[Dict]:
    """
    –ü–æ–ª—É—á–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –≤—Å–µ—Ö RSS —Ñ–∏–¥–æ–≤
    
    Returns:
        –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
    """
    all_news = []
    
    for feed_name, feed_config in RSS_FEEDS.items():
        logger.info(f"üì° –ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ {feed_name}...")
        try:
            news = parse_rss_feed(feed_config)
            all_news.extend(news)
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ {feed_name}: {e}")
    
    logger.info(f"‚úÖ –í—Å–µ–≥–æ –ø–æ–ª—É—á–µ–Ω–æ {len(all_news)} –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ RSS —Ñ–∏–¥–æ–≤")
    return all_news


def save_news_to_db(news_items: List[Dict], check_duplicates: bool = True):
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
    
    Args:
        news_items: –°–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        check_duplicates: –ü—Ä–æ–≤–µ—Ä—è—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ link
    """
    if not news_items:
        logger.info("‚ÑπÔ∏è –ù–µ—Ç –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
        return
    
    db_url = get_database_url()
    engine = create_engine(db_url)
    
    saved_count = 0
    skipped_count = 0
    
    with engine.begin() as conn:
        for item in news_items:
            try:
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ link (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞ –∏ link –µ—Å—Ç—å)
                if check_duplicates and item.get('link'):
                    existing = conn.execute(
                        text("""
                            SELECT id FROM knowledge_base 
                            WHERE link = :link
                        """),
                        {"link": item['link']}
                    ).fetchone()
                    
                    if existing:
                        skipped_count += 1
                        continue
                
                # –í—Å—Ç–∞–≤–ª—è–µ–º –Ω–æ–≤–æ—Å—Ç—å
                conn.execute(
                    text("""
                        INSERT INTO knowledge_base 
                        (ts, ticker, source, content, event_type, region, importance, link)
                        VALUES (:ts, :ticker, :source, :content, :event_type, :region, :importance, :link)
                    """),
                    {
                        "ts": item['published'],
                        "ticker": item['ticker'],
                        "source": item['source'],
                        "content": f"{item['title']}\n\n{item['content']}\n\nLink: {item['link']}" if item.get('link') else f"{item['title']}\n\n{item['content']}",
                        "event_type": item.get('event_type'),
                        "region": item.get('region'),
                        "importance": item.get('importance'),
                        "link": item.get('link', '')
                    }
                )
                saved_count += 1
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –Ω–æ–≤–æ—Å—Ç–∏ '{item.get('title', '')[:50]}...': {e}")
    
    logger.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {saved_count} –Ω–æ–≤–æ—Å—Ç–µ–π, –ø—Ä–æ–ø—É—â–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {skipped_count}")
    engine.dispose()


def fetch_and_save_rss_news():
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è: –ø–æ–ª—É—á–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ RSS –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ –ë–î
    """
    logger.info("üöÄ –ù–∞—á–∞–ª–æ –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ RSS —Ñ–∏–¥–æ–≤ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã—Ö –±–∞–Ω–∫–æ–≤")
    
    # –ü–æ–ª—É—á–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏
    news_items = fetch_all_rss_feeds()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
    if news_items:
        save_news_to_db(news_items)
    
    logger.info("‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ –ø–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ RSS —Ñ–∏–¥–æ–≤")


if __name__ == "__main__":
    import logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    fetch_and_save_rss_news()
