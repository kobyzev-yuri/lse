"""
–ú–æ–¥—É–ª—å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π (Vector Knowledge Base)
Embeddings: –ª–æ–∫–∞–ª—å–Ω–æ (sentence-transformers), —á–µ—Ä–µ–∑ OpenAI (—Ç–æ—Ç –∂–µ –∫–ª—é—á/proxy —á—Ç–æ GPT-4o) –∏–ª–∏ Gemini.
–ü—Ä–∏ Bus error / core dumped –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ USE_OPENAI_EMBEDDINGS=true (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø—Ä–∏ —É–∂–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω–æ–º GPT-4o).
"""

import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø—Ä–æ–µ–∫—Ç–∞ –≤ –ø—É—Ç—å
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import logging
from typing import List, Dict, Optional, Tuple, Any
from datetime import datetime, timedelta
import pandas as pd
from sqlalchemy import create_engine, text
import numpy as np

from config_loader import get_database_url, get_config_value

logger = logging.getLogger(__name__)

# –ú–æ–¥–µ–ª—å –¥–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö embeddings (768 dim). –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è —á–µ—Ä–µ–∑ HF_MODEL_NAME –≤ config.env.
DEFAULT_EMBEDDING_MODEL_NAME = "sentence-transformers/all-mpnet-base-v2"
EMBEDDING_DIMENSION = 768


def _get_local_embedding_model_name() -> str:
    """–ú–æ–¥–µ–ª—å –¥–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: –∏–∑ HF_MODEL_NAME –∏–ª–∏ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é."""
    name = (get_config_value("HF_MODEL_NAME") or "").strip()
    return name or DEFAULT_EMBEDDING_MODEL_NAME

# OpenAI: —Ç–æ—Ç –∂–µ OPENAI_API_KEY –∏ OPENAI_BASE_URL, —á—Ç–æ –¥–ª—è LLM (proxyapi). dimensions=768 –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –ë–î.
OPENAI_EMBED_MODEL = "text-embedding-3-small"

# Gemini (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞)
GEMINI_EMBED_MODEL = "text-embedding-004"
GEMINI_EMBED_URL = "https://generativelanguage.googleapis.com/v1beta/models/{model}:embedContent"


def _use_openai_embeddings() -> bool:
    v = get_config_value("USE_OPENAI_EMBEDDINGS") or ""
    return v.strip().lower() in ("1", "true", "yes")


def _use_gemini_embeddings() -> bool:
    v = get_config_value("USE_GEMINI_EMBEDDINGS") or ""
    return v.strip().lower() in ("1", "true", "yes")


def _get_openai_embed_config() -> Tuple[Optional[str], Optional[str]]:
    key = (get_config_value("OPENAI_API_KEY") or "").strip()
    base = (get_config_value("OPENAI_BASE_URL") or "https://api.proxyapi.ru/openai/v1").strip().rstrip("/")
    return (key or None, base or None)


def _get_gemini_api_key() -> Optional[str]:
    key = get_config_value("GEMINI_API_KEY") or ""
    return key.strip() or None


class VectorKB:
    """
    –ö–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π.
    Embeddings: OpenAI (—Ç–æ—Ç –∂–µ –∫–ª—é—á —á—Ç–æ GPT-4o), Gemini –∏–ª–∏ –ª–æ–∫–∞–ª—å–Ω–æ (sentence-transformers).
    """

    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è VectorKB"""
        self.db_url = get_database_url()
        self.engine = create_engine(self.db_url)

        self._use_openai = _use_openai_embeddings()
        self._openai_key, self._openai_base = _get_openai_embed_config() if self._use_openai else (None, None)
        self._use_gemini = _use_gemini_embeddings()
        self._gemini_key = _get_gemini_api_key() if self._use_gemini else None

        if self._use_openai and not self._openai_key:
            logger.warning("‚ö†Ô∏è USE_OPENAI_EMBEDDINGS=true, –Ω–æ OPENAI_API_KEY –Ω–µ –∑–∞–¥–∞–Ω ‚Äî —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –±—É–¥—É—Ç –Ω—É–ª–µ–≤—ã–º–∏")
        if self._use_gemini and not self._gemini_key:
            logger.warning("‚ö†Ô∏è USE_GEMINI_EMBEDDINGS=true, –Ω–æ GEMINI_API_KEY –Ω–µ –∑–∞–¥–∞–Ω ‚Äî —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –±—É–¥—É—Ç –Ω—É–ª–µ–≤—ã–º–∏")

        self._model = None
        self._model_loaded = False
        self._local_model_name = _get_local_embedding_model_name()

        if self._use_openai and self._openai_key:
            logger.info(f"‚úÖ VectorKB –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (–ø—Ä–æ–≤–∞–π–¥–µ—Ä: OpenAI, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {EMBEDDING_DIMENSION})")
        elif self._use_gemini and self._gemini_key:
            logger.info(f"‚úÖ VectorKB –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (–ø—Ä–æ–≤–∞–π–¥–µ—Ä: Gemini API, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {EMBEDDING_DIMENSION})")
        else:
            logger.info(f"‚úÖ VectorKB –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (–º–æ–¥–µ–ª—å: {self._local_model_name}, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {EMBEDDING_DIMENSION})")

    def _embed_openai(self, text: str) -> List[float]:
        """–≠–º–±–µ–¥–¥–∏–Ω–≥ —á–µ—Ä–µ–∑ OpenAI API (—Ç–æ—Ç –∂–µ –∫–ª—é—á –∏ base URL —á—Ç–æ –¥–ª—è GPT-4o). dimensions=768 –ø–æ–¥ –∫–æ–ª–æ–Ω–∫—É –ë–î."""
        import requests
        url = f"{self._openai_base}/embeddings"
        payload = {
            "model": OPENAI_EMBED_MODEL,
            "input": text[:8000],  # –ª–∏–º–∏—Ç –ø–æ —Ç–æ–∫–µ–Ω–∞–º
            "dimensions": EMBEDDING_DIMENSION,
        }
        try:
            r = requests.post(
                url,
                headers={
                    "Content-Type": "application/json",
                    "Authorization": f"Bearer {self._openai_key}",
                },
                json=payload,
                timeout=30,
            )
            r.raise_for_status()
            data = r.json()
            emb = data.get("data", [{}])[0].get("embedding")
            if not emb or len(emb) != EMBEDDING_DIMENSION:
                logger.error(f"‚ùå OpenAI –≤–µ—Ä–Ω—É–ª –Ω–µ–≤–µ—Ä–Ω—ã–π embedding: {len(emb or [])} dim")
                return [0.0] * EMBEDDING_DIMENSION
            arr = np.array(emb, dtype=np.float64)
            norm = np.linalg.norm(arr)
            if norm > 1e-9:
                arr = arr / norm
            return arr.tolist()
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ OpenAI Embedding API: {e}")
            return [0.0] * EMBEDDING_DIMENSION

    def _embed_gemini(self, text: str) -> List[float]:
        """–≠–º–±–µ–¥–¥–∏–Ω–≥ —á–µ—Ä–µ–∑ Gemini REST API (outputDimensionality=768)."""
        import requests
        url = GEMINI_EMBED_URL.format(model=GEMINI_EMBED_MODEL)
        payload = {
            "content": {"parts": [{"text": text[:20000]}]},  # –ª–∏–º–∏—Ç –ø–æ –¥–ª–∏–Ω–µ
            "outputDimensionality": EMBEDDING_DIMENSION,
        }
        try:
            r = requests.post(
                url,
                params={"key": self._gemini_key},
                json=payload,
                timeout=30,
                headers={"Content-Type": "application/json"},
            )
            r.raise_for_status()
            data = r.json()
            values = data.get("embedding", {}).get("values")
            if not values or len(values) != EMBEDDING_DIMENSION:
                logger.error(f"‚ùå Gemini –≤–µ—Ä–Ω—É–ª –Ω–µ–≤–µ—Ä–Ω—ã–π embedding: {len(values or [])} dim")
                return [0.0] * EMBEDDING_DIMENSION
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–∞–∫ —É sentence-transformers (L2)
            arr = np.array(values, dtype=np.float64)
            norm = np.linalg.norm(arr)
            if norm > 1e-9:
                arr = arr / norm
            return arr.tolist()
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ Gemini Embedding API: {e}")
            return [0.0] * EMBEDDING_DIMENSION

    def _load_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å sentence-transformers (–ª–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞). –ü—Ä–æ–∫—Å–∏ –æ—Ç–∫–ª—é—á–∞–µ—Ç—Å—è –Ω–∞ –≤—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏."""
        if self._model_loaded:
            return

        import os
        proxy_vars = (
            "HTTP_PROXY", "HTTPS_PROXY", "http_proxy", "https_proxy",
            "ALL_PROXY", "all_proxy", "SOCKS_PROXY", "socks_proxy", "NO_PROXY", "no_proxy"
        )
        saved = {k: os.environ.pop(k, None) for k in proxy_vars}
        try:
            from sentence_transformers import SentenceTransformer
            logger.info(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {self._local_model_name}...")
            self._model = SentenceTransformer(self._local_model_name)
            self._model_loaded = True
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {self._local_model_name} –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        except ImportError:
            logger.error("‚ùå sentence-transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install sentence-transformers")
            raise
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            raise
        finally:
            for k, v in saved.items():
                if v is not None:
                    os.environ[k] = v

    def _maybe_e5_prefix(self, text: str, for_query: bool) -> str:
        """–î–ª—è –º–æ–¥–µ–ª–µ–π E5 (multilingual-e5-base –∏ –¥—Ä.) –¥–æ–±–∞–≤–ª—è–µ—Ç –ø—Ä–µ—Ñ–∏–∫—Å query: / passage:."""
        if "e5" in self._local_model_name.lower():
            prefix = "query: " if for_query else "passage: "
            return prefix + text if text else text
        return text

    def generate_embedding(self, text: str, for_query: bool = False) -> List[float]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç embedding –¥–ª—è —Ç–µ–∫—Å—Ç–∞ (OpenAI, Gemini –∏–ª–∏ –ª–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å).
        for_query: True –¥–ª—è –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ (—É E5 ‚Äî –ø—Ä–µ—Ñ–∏–∫—Å "query: "), False –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ("passage: ").
        Returns:
            –°–ø–∏—Å–æ–∫ –∏–∑ 768 —á–∏—Å–µ–ª (embedding).
        """
        if not text or not text.strip():
            logger.warning("‚ö†Ô∏è –ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ embedding, –≤–æ–∑–≤—Ä–∞—â–∞—é –Ω—É–ª–µ–≤–æ–π –≤–µ–∫—Ç–æ—Ä")
            return [0.0] * EMBEDDING_DIMENSION

        if self._use_openai and self._openai_key:
            return self._embed_openai(text)
        if self._use_gemini and self._gemini_key:
            return self._embed_gemini(text)

        self._load_model()
        text_to_encode = self._maybe_e5_prefix(text, for_query)

        try:
            embedding = self._model.encode(text_to_encode, normalize_embeddings=True)
            embedding_list = embedding.tolist()
            if len(embedding_list) != EMBEDDING_DIMENSION:
                logger.error(f"‚ùå –ù–µ–≤–µ—Ä–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å embedding: {len(embedding_list)}, –æ–∂–∏–¥–∞–µ—Ç—Å—è {EMBEDDING_DIMENSION}")
                return [0.0] * EMBEDDING_DIMENSION
            return embedding_list
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ embedding: {e}")
            return [0.0] * EMBEDDING_DIMENSION
    
    def add_event(
        self,
        ticker: str,
        event_type: str,
        content: str,
        ts: datetime,
        source: Optional[str] = None,
        knowledge_base_id: Optional[int] = None,
    ) -> Optional[int]:
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç —Å–æ–±—ã—Ç–∏–µ –≤ knowledge_base —Å embedding (–æ–¥–Ω–∞ —Ç–∞–±–ª–∏—Ü–∞ –¥–ª—è –Ω–æ–≤–æ—Å—Ç–µ–π –∏ –≤–µ–∫—Ç–æ—Ä–æ–≤).
        
        Args:
            ticker: –¢–∏–∫–µ—Ä –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞
            event_type: –¢–∏–ø —Å–æ–±—ã—Ç–∏—è ('NEWS', 'EARNINGS', 'ECONOMIC_INDICATOR', 'TRADE_SIGNAL')
            content: –¢–µ–∫—Å—Ç —Å–æ–±—ã—Ç–∏—è
            ts: –í—Ä–µ–º–µ–Ω–Ω–∞—è –º–µ—Ç–∫–∞
            source: –ò—Å—Ç–æ—á–Ω–∏–∫ (—Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –≤ –ë–î; –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 'MANUAL')
            knowledge_base_id: –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è (–æ—Å—Ç–∞–≤–ª–µ–Ω –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ API)
            
        Returns:
            ID –∑–∞–ø–∏—Å–∏ –≤ knowledge_base –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        if not content or not content.strip():
            logger.warning(f"‚ö†Ô∏è –ü—É—Å—Ç–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è —Å–æ–±—ã—Ç–∏—è {ticker}, –ø—Ä–æ–ø—É—Å–∫")
            return None
        
        try:
            embedding = self.generate_embedding(content)
            src = (source or "MANUAL").strip() or "MANUAL"
            with self.engine.begin() as conn:
                result = conn.execute(
                    text("""
                        INSERT INTO knowledge_base (ts, ticker, source, content, event_type, embedding)
                        VALUES (:ts, :ticker, :source, :content, :event_type, :embedding)
                        RETURNING id
                    """),
                    {
                        "ts": ts,
                        "ticker": ticker,
                        "source": src,
                        "content": content,
                        "event_type": event_type,
                        "embedding": f"[{','.join(map(str, embedding))}]",
                    },
                )
                event_id = result.fetchone()[0]
                logger.debug(f"‚úÖ –°–æ–±—ã—Ç–∏–µ –¥–æ–±–∞–≤–ª–µ–Ω–æ –≤ knowledge_base: id={event_id}, ticker={ticker}")
                return event_id
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Å–æ–±—ã—Ç–∏—è –≤ knowledge_base: {e}")
            return None
    
    def search_similar(
        self,
        query: str,
        ticker: Optional[str] = None,
        limit: int = 5,
        min_similarity: float = 0.5,
        time_window_days: int = 365,
        event_types: Optional[List[str]] = None
    ) -> pd.DataFrame:
        """
        –ò—â–µ—Ç –ø–æ—Ö–æ–∂–∏–µ —Å–æ–±—ã—Ç–∏—è —á–µ—Ä–µ–∑ –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫
        
        Args:
            query: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞
            ticker: –§–∏–ª—å—Ç—Ä –ø–æ —Ç–∏–∫–µ—Ä—É (–µ—Å–ª–∏ None - –≤—Å–µ —Ç–∏–∫–µ—Ä—ã)
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            min_similarity: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è similarity (0.0-1.0)
            time_window_days: –û–∫–Ω–æ –ø–æ–∏—Å–∫–∞ –≤ –¥–Ω—è—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1 –≥–æ–¥)
            event_types: –°–ø–∏—Å–æ–∫ —Ç–∏–ø–æ–≤ —Å–æ–±—ã—Ç–∏–π –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (–µ—Å–ª–∏ None - –≤—Å–µ)
            
        Returns:
            DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏: id, ticker, event_type, content, ts, similarity
        """
        if not query or not query.strip():
            logger.warning("‚ö†Ô∏è –ü—É—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞")
            return pd.DataFrame()
        
        try:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º embedding –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ (for_query=True –¥–ª—è E5)
            query_embedding = self.generate_embedding(query, for_query=True)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º SQL –∑–∞–ø—Ä–æ—Å
            where_clauses = []
            params = {
                "query_embedding": f"[{','.join(map(str, query_embedding))}]",  # pgvector —Ñ–æ—Ä–º–∞—Ç
                "limit": limit,
                "min_similarity": min_similarity,
                "cutoff_time": datetime.now() - timedelta(days=time_window_days)
            }
            
            # –§–∏–ª—å—Ç—Ä –ø–æ –≤—Ä–µ–º–µ–Ω–∏
            where_clauses.append("ts >= :cutoff_time")
            
            # –§–∏–ª—å—Ç—Ä –ø–æ —Ç–∏–∫–µ—Ä—É
            if ticker:
                where_clauses.append("(ticker = :ticker OR ticker IN ('MACRO', 'US_MACRO'))")
                params["ticker"] = ticker
            else:
                where_clauses.append("(ticker IS NOT NULL)")
            
            # –§–∏–ª—å—Ç—Ä –ø–æ —Ç–∏–ø–∞–º —Å–æ–±—ã—Ç–∏–π
            if event_types:
                placeholders = ','.join([f"'{et}'" for et in event_types])
                where_clauses.append(f"event_type IN ({placeholders})")
            
            where_sql = " AND ".join(where_clauses)
            
            # –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ pgvector (cosine distance)
            # –û–ø–µ—Ä–∞—Ç–æ—Ä <=> –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç cosine distance (0 = –∏–¥–µ–Ω—Ç–∏—á–Ω—ã, 2 = –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω—ã)
            # similarity = 1 - distance (1 = –∏–¥–µ–Ω—Ç–∏—á–Ω—ã, -1 = –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω—ã)
            query_sql = f"""
                SELECT 
                    id, ticker, event_type, content, ts,
                    1 - (embedding <=> CAST(:query_embedding AS vector)) as similarity
                FROM knowledge_base
                WHERE embedding IS NOT NULL AND {where_sql}
                  AND (1 - (embedding <=> CAST(:query_embedding AS vector))) >= :min_similarity
                ORDER BY embedding <=> CAST(:query_embedding AS vector)
                LIMIT :limit
            """
            
            with self.engine.connect() as conn:
                df = pd.read_sql(text(query_sql), conn, params=params)
            
            if df.empty:
                logger.info(f"‚ÑπÔ∏è –ü–æ—Ö–æ–∂–∏—Ö —Å–æ–±—ã—Ç–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: {query[:50]}...")
            else:
                logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(df)} –ø–æ—Ö–æ–∂–∏—Ö —Å–æ–±—ã—Ç–∏–π (similarity >= {min_similarity:.2f})")
            
            return df
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
            return pd.DataFrame()
    
    def count_without_embedding(self) -> int:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —á–∏—Å–ª–æ –∑–∞–ø–∏—Å–µ–π –±–µ–∑ embedding —Å –ø–æ–¥—Ö–æ–¥—è—â–∏–º content (–¥–ª—è backfill)."""
        try:
            with self.engine.connect() as conn:
                row = conn.execute(
                    text("""
                        SELECT COUNT(*) FROM knowledge_base
                        WHERE embedding IS NULL
                          AND content IS NOT NULL
                          AND TRIM(content) != ''
                          AND LENGTH(TRIM(content)) > 10
                    """)
                ).fetchone()
                return row[0] if row else 0
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥—Å—á—ë—Ç–∞ –∑–∞–ø–∏—Å–µ–π –±–µ–∑ embedding: {e}")
            return 0

    def count_total_without_embedding(self) -> int:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—â–µ–µ —á–∏—Å–ª–æ –∑–∞–ø–∏—Å–µ–π –±–µ–∑ embedding (–±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–∞ –ø–æ content)."""
        try:
            with self.engine.connect() as conn:
                row = conn.execute(
                    text("SELECT COUNT(*) FROM knowledge_base WHERE embedding IS NULL")
                ).fetchone()
                return row[0] if row else 0
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥—Å—á—ë—Ç–∞: {e}")
            return 0

    def sync_from_knowledge_base(self, limit: Optional[int] = None, batch_size: int = 100):
        """
        –ü—Ä–æ—Å—Ç–∞–≤–ª—è–µ—Ç embedding –≤ knowledge_base –¥–ª—è –∑–∞–ø–∏—Å–µ–π, —É –∫–æ—Ç–æ—Ä—ã—Ö –æ–Ω –µ—â—ë –Ω–µ –∑–∞–ø–æ–ª–Ω–µ–Ω.
        –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–∫–æ–ª—å–∫–æ —Ç–∞–∫–∏—Ö –∑–∞–ø–∏—Å–µ–π –µ—Å—Ç—å; –∑–∞—Ç–µ–º –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –±–∞—Ç—á–∞–º–∏.
        
        Args:
            limit: –ú–∞–∫—Å–∏–º—É–º –∑–∞–ø–∏—Å–µ–π –∑–∞ –∑–∞–ø—É—Å–∫ (None ‚Äî –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—Å–µ –±–µ–∑ –ª–∏–º–∏—Ç–∞)
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
        """
        logger.info("üîÑ Backfill embedding: –ø—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–ø–∏—Å–µ–π –±–µ–∑ embedding...")

        try:
            # 1. –Ø–≤–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: —Å–∫–æ–ª—å–∫–æ –∑–∞–ø–∏—Å–µ–π –±–µ–∑ embedding
            total_without = self.count_total_without_embedding()
            need_count = self.count_without_embedding()
            skipped_content = total_without - need_count
            logger.info(f"üìä –í—Å–µ–≥–æ –±–µ–∑ embedding: {total_without}. –ö –æ–±—Ä–∞–±–æ—Ç–∫–µ (content –Ω–µ –ø—É—Å—Ç–æ–π, –¥–ª–∏–Ω–∞ > 10): {need_count}")
            if skipped_content > 0:
                logger.info(f"   –ü—Ä–æ–ø—É—â–µ–Ω–æ –∏–∑-–∑–∞ –ø—É—Å—Ç–æ–≥–æ –∏–ª–∏ –∫–æ—Ä–æ—Ç–∫–æ–≥–æ content (‚â§10 —Å–∏–º–≤–æ–ª–æ–≤): {skipped_content}")
            if need_count == 0:
                logger.info("‚ÑπÔ∏è –ù–µ—á–µ–≥–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ.")
                return

            # 2. –í—ã–±–æ—Ä–∫–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–±–µ–∑ –ª–∏–º–∏—Ç–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é; LIMIT NULL –≤ PostgreSQL = –≤—Å–µ —Å—Ç—Ä–æ–∫–∏)
            with self.engine.connect() as conn:
                query = text("""
                    SELECT id, ticker, content, event_type
                    FROM knowledge_base
                    WHERE embedding IS NULL
                      AND content IS NOT NULL
                      AND TRIM(content) != ''
                      AND LENGTH(TRIM(content)) > 10
                    ORDER BY id
                    LIMIT :lim
                """)
                df = pd.read_sql(query, conn, params={"lim": limit})
            
            to_process = len(df)
            logger.info(f"üìä –ö –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤ —ç—Ç–æ–º –∑–∞–ø—É—Å–∫–µ: {to_process}" + (f" (–ª–∏–º–∏—Ç {limit})" if limit is not None else " (–±–µ–∑ –ª–∏–º–∏—Ç–∞)"))
            if to_process == 0:
                return

            updated_count = 0
            error_count = 0
            first_error = None

            for i in range(0, to_process, batch_size):
                batch = df.iloc[i : i + batch_size]
                for _, row in batch.iterrows():
                    try:
                        emb = self.generate_embedding(row["content"])
                        emb_str = f"[{','.join(map(str, emb))}]"
                        with self.engine.begin() as conn:
                            conn.execute(
                                text("UPDATE knowledge_base SET embedding = CAST(:emb AS vector) WHERE id = :id"),
                                {"emb": emb_str, "id": int(row["id"])},
                            )
                        updated_count += 1
                    except Exception as e:
                        error_count += 1
                        if first_error is None:
                            first_error = e
                        logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ backfill id={row['id']}: {e}")
                logger.info(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {min(i + batch_size, to_process)}/{to_process}")
            
            if first_error is not None and error_count > 0:
                logger.warning(f"‚ö†Ô∏è –ü–µ—Ä–≤–∞—è –æ—à–∏–±–∫–∞ (–¥–ª—è –æ—Ç–ª–∞–¥–∫–∏): {first_error}", exc_info=False)
            logger.info(f"‚úÖ Backfill –∑–∞–≤–µ—Ä—à—ë–Ω: –æ–±–Ω–æ–≤–ª–µ–Ω–æ {updated_count}, –æ—à–∏–±–æ–∫ {error_count}")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ backfill: {e}", exc_info=True)
    
    def get_stats(self) -> Dict[str, Any]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∑–∞–ø–∏—Å—è–º —Å embedding –≤ knowledge_base.
        """
        try:
            with self.engine.connect() as conn:
                total = conn.execute(text("SELECT COUNT(*) FROM knowledge_base")).fetchone()[0]
                with_embedding = conn.execute(
                    text("SELECT COUNT(*) FROM knowledge_base WHERE embedding IS NOT NULL")
                ).fetchone()[0]
                without_total = self.count_total_without_embedding()
                without_ready = self.count_without_embedding()
                by_type = {}
                result = conn.execute(
                    text("SELECT event_type, COUNT(*) FROM knowledge_base WHERE embedding IS NOT NULL GROUP BY event_type")
                )
                for row in result:
                    by_type[row[0] or "NULL"] = row[1]
                return {
                    "total_events": total,
                    "with_embedding": with_embedding,
                    "without_embedding": without_total,
                    "without_embedding_ready": without_ready,
                    "without_embedding_skipped_content": without_total - without_ready,
                    "by_event_type": by_type,
                }
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
            return {}


if __name__ == "__main__":
    import logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    # –¢–µ—Å—Ç
    vector_kb = VectorKB()
    
    # –¢–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ embedding
    test_text = "Microsoft –æ–±—ä—è–≤–∏–ª –æ —Ä–æ—Å—Ç–µ –≤—ã—Ä—É—á–∫–∏ –Ω–∞ 15% –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–º –∫–≤–∞—Ä—Ç–∞–ª–µ"
    embedding = vector_kb.generate_embedding(test_text)
    print(f"‚úÖ Embedding —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω: —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å {len(embedding)}")
    
    # –¢–µ—Å—Ç –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Å–æ–±—ã—Ç–∏—è
    event_id = vector_kb.add_event(
        ticker="MSFT",
        event_type="NEWS",
        content=test_text,
        ts=datetime.now()
    )
    print(f"‚úÖ –°–æ–±—ã—Ç–∏–µ –¥–æ–±–∞–≤–ª–µ–Ω–æ: ID={event_id}")
    
    # –¢–µ—Å—Ç –ø–æ–∏—Å–∫–∞
    similar = vector_kb.search_similar("Microsoft –≤—ã—Ä—É—á–∫–∞ —Ä–æ—Å—Ç", ticker="MSFT", limit=3)
    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –ø–æ—Ö–æ–∂–∏—Ö —Å–æ–±—ã—Ç–∏–π: {len(similar)}")
    if not similar.empty:
        print(similar[['ticker', 'event_type', 'similarity', 'content']].head())
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    stats = vector_kb.get_stats()
    print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {stats}")
