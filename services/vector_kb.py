"""
–ú–æ–¥—É–ª—å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π (Vector Knowledge Base)
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç sentence-transformers –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ embeddings –ª–æ–∫–∞–ª—å–Ω–æ (–±–µ—Å–ø–ª–∞—Ç–Ω–æ)
"""

import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø—Ä–æ–µ–∫—Ç–∞ –≤ –ø—É—Ç—å
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import logging
from typing import List, Dict, Optional, Tuple
from datetime import datetime, timedelta
import pandas as pd
from sqlalchemy import create_engine, text
import numpy as np

from config_loader import get_database_url

logger = logging.getLogger(__name__)

# –ú–æ–¥–µ–ª—å –¥–ª—è embeddings (all-mpnet-base-v2: 768 dim, –ø–æ–ø—É–ª—è—Ä–Ω–∞—è, –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è)
EMBEDDING_MODEL_NAME = "sentence-transformers/all-mpnet-base-v2"
EMBEDDING_DIMENSION = 768


class VectorKB:
    """
    –ö–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç sentence-transformers –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ embeddings –ª–æ–∫–∞–ª—å–Ω–æ (–±–µ—Å–ø–ª–∞—Ç–Ω–æ).
    –ú–æ–¥–µ–ª—å: all-mpnet-base-v2 (768 –∏–∑–º–µ—Ä–µ–Ω–∏–π) - –ø–æ–ø—É–ª—è—Ä–Ω–∞—è –º–æ–¥–µ–ª—å —Å —Ö–æ—Ä–æ—à–∏–º –∫–∞—á–µ—Å—Ç–≤–æ–º.
    """
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è VectorKB"""
        self.db_url = get_database_url()
        self.engine = create_engine(self.db_url)
        
        # –õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ (–∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏)
        self._model = None
        self._model_loaded = False
        
        logger.info(f"‚úÖ VectorKB –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (–º–æ–¥–µ–ª—å: {EMBEDDING_MODEL_NAME}, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {EMBEDDING_DIMENSION})")
    
    def _load_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å sentence-transformers (–ª–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞)"""
        if self._model_loaded:
            return
        
        try:
            from sentence_transformers import SentenceTransformer
            logger.info(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {EMBEDDING_MODEL_NAME}...")
            self._model = SentenceTransformer(EMBEDDING_MODEL_NAME)
            self._model_loaded = True
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {EMBEDDING_MODEL_NAME} –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        except ImportError:
            logger.error("‚ùå sentence-transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install sentence-transformers")
            raise
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            raise
    
    def generate_embedding(self, text: str) -> List[float]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç embedding –¥–ª—è —Ç–µ–∫—Å—Ç–∞
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
            
        Returns:
            –°–ø–∏—Å–æ–∫ –∏–∑ 768 —á–∏—Å–µ–ª (embedding)
        """
        if not text or not text.strip():
            logger.warning("‚ö†Ô∏è –ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ embedding, –≤–æ–∑–≤—Ä–∞—â–∞—é –Ω—É–ª–µ–≤–æ–π –≤–µ–∫—Ç–æ—Ä")
            return [0.0] * EMBEDDING_DIMENSION
        
        self._load_model()
        
        try:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º embedding
            embedding = self._model.encode(text, normalize_embeddings=True)
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º numpy array –≤ —Å–ø–∏—Å–æ–∫
            embedding_list = embedding.tolist()
            
            if len(embedding_list) != EMBEDDING_DIMENSION:
                logger.error(f"‚ùå –ù–µ–≤–µ—Ä–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å embedding: {len(embedding_list)}, –æ–∂–∏–¥–∞–µ—Ç—Å—è {EMBEDDING_DIMENSION}")
                return [0.0] * EMBEDDING_DIMENSION
            
            return embedding_list
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ embedding: {e}")
            return [0.0] * EMBEDDING_DIMENSION
    
    def add_event(
        self,
        ticker: str,
        event_type: str,
        content: str,
        ts: datetime,
        source: Optional[str] = None
    ) -> Optional[int]:
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç —Å–æ–±—ã—Ç–∏–µ –≤ trade_kb —Å embedding
        
        Args:
            ticker: –¢–∏–∫–µ—Ä –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞
            event_type: –¢–∏–ø —Å–æ–±—ã—Ç–∏—è ('NEWS', 'EARNINGS', 'ECONOMIC_INDICATOR', 'TRADE_SIGNAL')
            content: –¢–µ–∫—Å—Ç —Å–æ–±—ã—Ç–∏—è
            ts: –í—Ä–µ–º–µ–Ω–Ω–∞—è –º–µ—Ç–∫–∞
            source: –ò—Å—Ç–æ—á–Ω–∏–∫ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            
        Returns:
            ID –¥–æ–±–∞–≤–ª–µ–Ω–Ω–æ–π –∑–∞–ø–∏—Å–∏ –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        if not content or not content.strip():
            logger.warning(f"‚ö†Ô∏è –ü—É—Å—Ç–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è —Å–æ–±—ã—Ç–∏—è {ticker}, –ø—Ä–æ–ø—É—Å–∫")
            return None
        
        try:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º embedding
            embedding = self.generate_embedding(content)
            
            # –í—Å—Ç–∞–≤–ª—è–µ–º –≤ –ë–î
            with self.engine.begin() as conn:
                result = conn.execute(
                    text("""
                        INSERT INTO trade_kb (ts, ticker, event_type, content, embedding)
                        VALUES (:ts, :ticker, :event_type, :content, :embedding)
                        RETURNING id
                    """),
                    {
                        "ts": ts,
                        "ticker": ticker,
                        "event_type": event_type,
                        "content": content,
                        "embedding": f"[{','.join(map(str, embedding))}]"  # pgvector —Ñ–æ—Ä–º–∞—Ç: [1,2,3,...]
                    }
                )
                event_id = result.fetchone()[0]
                logger.debug(f"‚úÖ –°–æ–±—ã—Ç–∏–µ –¥–æ–±–∞–≤–ª–µ–Ω–æ –≤ trade_kb: ID={event_id}, ticker={ticker}, type={event_type}")
                return event_id
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Å–æ–±—ã—Ç–∏—è –≤ trade_kb: {e}")
            return None
    
    def search_similar(
        self,
        query: str,
        ticker: Optional[str] = None,
        limit: int = 5,
        min_similarity: float = 0.5,
        time_window_days: int = 365,
        event_types: Optional[List[str]] = None
    ) -> pd.DataFrame:
        """
        –ò—â–µ—Ç –ø–æ—Ö–æ–∂–∏–µ —Å–æ–±—ã—Ç–∏—è —á–µ—Ä–µ–∑ –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫
        
        Args:
            query: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞
            ticker: –§–∏–ª—å—Ç—Ä –ø–æ —Ç–∏–∫–µ—Ä—É (–µ—Å–ª–∏ None - –≤—Å–µ —Ç–∏–∫–µ—Ä—ã)
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            min_similarity: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è similarity (0.0-1.0)
            time_window_days: –û–∫–Ω–æ –ø–æ–∏—Å–∫–∞ –≤ –¥–Ω—è—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1 –≥–æ–¥)
            event_types: –°–ø–∏—Å–æ–∫ —Ç–∏–ø–æ–≤ —Å–æ–±—ã—Ç–∏–π –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (–µ—Å–ª–∏ None - –≤—Å–µ)
            
        Returns:
            DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏: id, ticker, event_type, content, ts, similarity
        """
        if not query or not query.strip():
            logger.warning("‚ö†Ô∏è –ü—É—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞")
            return pd.DataFrame()
        
        try:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º embedding –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
            query_embedding = self.generate_embedding(query)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º SQL –∑–∞–ø—Ä–æ—Å
            where_clauses = []
            params = {
                "query_embedding": f"[{','.join(map(str, query_embedding))}]",  # pgvector —Ñ–æ—Ä–º–∞—Ç
                "limit": limit,
                "min_similarity": min_similarity,
                "cutoff_time": datetime.now() - timedelta(days=time_window_days)
            }
            
            # –§–∏–ª—å—Ç—Ä –ø–æ –≤—Ä–µ–º–µ–Ω–∏
            where_clauses.append("ts >= :cutoff_time")
            
            # –§–∏–ª—å—Ç—Ä –ø–æ —Ç–∏–∫–µ—Ä—É
            if ticker:
                where_clauses.append("(ticker = :ticker OR ticker IN ('MACRO', 'US_MACRO'))")
                params["ticker"] = ticker
            else:
                where_clauses.append("(ticker IS NOT NULL)")
            
            # –§–∏–ª—å—Ç—Ä –ø–æ —Ç–∏–ø–∞–º —Å–æ–±—ã—Ç–∏–π
            if event_types:
                placeholders = ','.join([f"'{et}'" for et in event_types])
                where_clauses.append(f"event_type IN ({placeholders})")
            
            where_sql = " AND ".join(where_clauses)
            
            # –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ pgvector (cosine distance)
            # –û–ø–µ—Ä–∞—Ç–æ—Ä <=> –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç cosine distance (0 = –∏–¥–µ–Ω—Ç–∏—á–Ω—ã, 2 = –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω—ã)
            # similarity = 1 - distance (1 = –∏–¥–µ–Ω—Ç–∏—á–Ω—ã, -1 = –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω—ã)
            query_sql = f"""
                SELECT 
                    id, ticker, event_type, content, ts,
                    1 - (embedding <=> CAST(:query_embedding AS vector)) as similarity
                FROM trade_kb
                WHERE {where_sql}
                  AND (1 - (embedding <=> CAST(:query_embedding AS vector))) >= :min_similarity
                ORDER BY embedding <=> CAST(:query_embedding AS vector)
                LIMIT :limit
            """
            
            with self.engine.connect() as conn:
                df = pd.read_sql(text(query_sql), conn, params=params)
            
            if df.empty:
                logger.info(f"‚ÑπÔ∏è –ü–æ—Ö–æ–∂–∏—Ö —Å–æ–±—ã—Ç–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: {query[:50]}...")
            else:
                logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(df)} –ø–æ—Ö–æ–∂–∏—Ö —Å–æ–±—ã—Ç–∏–π (similarity >= {min_similarity:.2f})")
            
            return df
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
            return pd.DataFrame()
    
    def sync_from_knowledge_base(self, limit: Optional[int] = None, batch_size: int = 100):
        """
        –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ knowledge_base –≤ trade_kb
        
        –î–ª—è –∫–∞–∂–¥–æ–π –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ knowledge_base, –∫–æ—Ç–æ—Ä–æ–π –Ω–µ—Ç –≤ trade_kb,
        –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç embedding –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –≤ trade_kb.
        
        Args:
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ (–µ—Å–ª–∏ None - –≤—Å–µ –Ω–æ–≤—ã–µ)
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        logger.info("üîÑ –ù–∞—á–∞–ª–æ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ knowledge_base ‚Üí trade_kb")
        
        synced_count = 0
        skipped_count = 0
        error_count = 0
        
        try:
            with self.engine.connect() as conn:
                # –ù–∞—Ö–æ–¥–∏–º –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ knowledge_base, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ trade_kb
                # –°–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ–º –ø–æ ticker, event_type, content –∏ ts (–ø—Ä–∏–º–µ—Ä–Ω–æ)
                query = text("""
                    SELECT DISTINCT ON (kb.id) 
                           kb.id, kb.ts, kb.ticker, kb.source, kb.content,
                           kb.event_type, kb.importance
                    FROM knowledge_base kb
                    WHERE NOT EXISTS (
                        SELECT 1 FROM trade_kb tk
                        WHERE tk.ticker = kb.ticker 
                          AND COALESCE(tk.event_type, 'NEWS') = COALESCE(kb.event_type, 'NEWS')
                          AND ABS(EXTRACT(EPOCH FROM (kb.ts - tk.ts))) < 3600  -- –í –ø—Ä–µ–¥–µ–ª–∞—Ö —á–∞—Å–∞
                          AND LEFT(kb.content, 100) = LEFT(tk.content, 100)  -- –ü–µ—Ä–≤—ã–µ 100 —Å–∏–º–≤–æ–ª–æ–≤ —Å–æ–≤–ø–∞–¥–∞—é—Ç
                    )
                      AND kb.content IS NOT NULL
                      AND LENGTH(kb.content) > 10
                    ORDER BY kb.id, kb.ts DESC
                    LIMIT :limit
                """)
                
                params = {"limit": limit if limit else 10000}
                news_df = pd.read_sql(query, conn, params=params)
                
                if news_df.empty:
                    logger.info("‚ÑπÔ∏è –ù–µ—Ç –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏")
                    return
                
                logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(news_df)} –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏")
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á–∞–º–∏
                for i in range(0, len(news_df), batch_size):
                    batch = news_df.iloc[i:i+batch_size]
                    
                    for _, row in batch.iterrows():
                        try:
                            event_type = row.get('event_type') or 'NEWS'
                            
                            event_id = self.add_event(
                                ticker=row['ticker'],
                                event_type=event_type,
                                content=row['content'],
                                ts=row['ts'],
                                source=row.get('source')
                            )
                            
                            if event_id:
                                synced_count += 1
                            else:
                                skipped_count += 1
                        except Exception as e:
                            error_count += 1
                            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–∏ ID={row['id']}: {e}")
                    
                    logger.info(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {min(i+batch_size, len(news_df))}/{len(news_df)} –Ω–æ–≤–æ—Å—Ç–µ–π")
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏: {e}")
        
        logger.info(
            f"‚úÖ –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: –¥–æ–±–∞–≤–ª–µ–Ω–æ {synced_count}, "
            f"–ø—Ä–æ–ø—É—â–µ–Ω–æ {skipped_count}, –æ—à–∏–±–æ–∫ {error_count}"
        )
    
    def get_stats(self) -> Dict[str, int]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ trade_kb
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
        """
        try:
            with self.engine.connect() as conn:
                total = conn.execute(text("SELECT COUNT(*) FROM trade_kb")).fetchone()[0]
                with_embedding = conn.execute(
                    text("SELECT COUNT(*) FROM trade_kb WHERE embedding IS NOT NULL")
                ).fetchone()[0]
                
                by_type = {}
                result = conn.execute(
                    text("SELECT event_type, COUNT(*) FROM trade_kb GROUP BY event_type")
                )
                for row in result:
                    by_type[row[0] or 'NULL'] = row[1]
                
                return {
                    'total_events': total,
                    'with_embedding': with_embedding,
                    'by_event_type': by_type
                }
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
            return {}


if __name__ == "__main__":
    import logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    # –¢–µ—Å—Ç
    vector_kb = VectorKB()
    
    # –¢–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ embedding
    test_text = "Microsoft –æ–±—ä—è–≤–∏–ª –æ —Ä–æ—Å—Ç–µ –≤—ã—Ä—É—á–∫–∏ –Ω–∞ 15% –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–º –∫–≤–∞—Ä—Ç–∞–ª–µ"
    embedding = vector_kb.generate_embedding(test_text)
    print(f"‚úÖ Embedding —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω: —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å {len(embedding)}")
    
    # –¢–µ—Å—Ç –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Å–æ–±—ã—Ç–∏—è
    event_id = vector_kb.add_event(
        ticker="MSFT",
        event_type="NEWS",
        content=test_text,
        ts=datetime.now()
    )
    print(f"‚úÖ –°–æ–±—ã—Ç–∏–µ –¥–æ–±–∞–≤–ª–µ–Ω–æ: ID={event_id}")
    
    # –¢–µ—Å—Ç –ø–æ–∏—Å–∫–∞
    similar = vector_kb.search_similar("Microsoft –≤—ã—Ä—É—á–∫–∞ —Ä–æ—Å—Ç", ticker="MSFT", limit=3)
    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –ø–æ—Ö–æ–∂–∏—Ö —Å–æ–±—ã—Ç–∏–π: {len(similar)}")
    if not similar.empty:
        print(similar[['ticker', 'event_type', 'similarity', 'content']].head())
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    stats = vector_kb.get_stats()
    print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {stats}")
