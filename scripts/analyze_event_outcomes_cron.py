#!/usr/bin/env python3
"""
Cron —Å–∫—Ä–∏–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏—Å—Ö–æ–¥–æ–≤ —Å–æ–±—ã—Ç–∏–π –≤ knowledge_base.
–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–æ–±—ã—Ç–∏—è, –∫–æ—Ç–æ—Ä—ã–º —É–∂–µ –ø—Ä–æ—à–ª–æ N –¥–Ω–µ–π, –∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç outcome_json.
"""

import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø—Ä–æ–µ–∫—Ç–∞ –≤ –ø—É—Ç—å
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
Path(project_root / "logs").mkdir(parents=True, exist_ok=True)

import logging
import os
from datetime import datetime, timedelta
from sqlalchemy import create_engine, text
import pandas as pd

from config_loader import get_database_url
from services.news_impact_analyzer import NewsImpactAnalyzer

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/analyze_event_outcomes.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)


def analyze_existing_events(
    days_after: int = 7,
    limit: int = None,
    batch_size: int = 50
):
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏—Å—Ö–æ–¥—ã —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å–æ–±—ã—Ç–∏–π –≤ knowledge_base
    
    Args:
        days_after: –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –ø–æ—Å–ª–µ —Å–æ–±—ã—Ç–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–±—ã—Ç–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–µ—Å–ª–∏ None - –≤—Å–µ –ø–æ–¥—Ö–æ–¥—è—â–∏–µ)
        batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
    """
    logger.info("=" * 60)
    logger.info("üîÑ –ù–∞—á–∞–ª–æ –∞–Ω–∞–ª–∏–∑–∞ –∏—Å—Ö–æ–¥–æ–≤ —Å–æ–±—ã—Ç–∏–π")
    logger.info("=" * 60)
    
    db_url = get_database_url()
    engine = create_engine(db_url)
    analyzer = NewsImpactAnalyzer()
    
    analyzed_count = 0
    skipped_count = 0
    error_count = 0
    updated_count = 0
    
    try:
        # –ù–∞—Ö–æ–¥–∏–º —Å–æ–±—ã—Ç–∏—è, –∫–æ—Ç–æ—Ä—ã–µ:
        # 1. –ü—Ä–æ–∏–∑–æ—à–ª–∏ –Ω–µ –º–µ–Ω–µ–µ N –¥–Ω–µ–π –Ω–∞–∑–∞–¥
        # 2. –ï—â—ë –Ω–µ –∏–º–µ—é—Ç outcome_json
        # 3. –ò–º–µ—é—Ç ticker –∏ content
        # 4. –¢–æ–ª—å–∫–æ —Ç–∏–∫–µ—Ä—ã, –ø–æ –∫–æ—Ç–æ—Ä—ã–º –µ—Å—Ç—å –∫–æ—Ç–∏—Ä–æ–≤–∫–∏ –≤ quotes (–∏–Ω–∞—á–µ ¬´–ù–µ—Ç –∫–æ—Ç–∏—Ä–æ–≤–æ–∫¬ª –ø–æ GOOGL, LOGI –∏ —Ç.–¥.)
        # 5. –ù–µ —Å—Ç–∞—Ä—à–µ 5 –ª–µ—Ç
        cutoff_date = datetime.now() - timedelta(days=days_after)
        min_date = datetime.now() - timedelta(days=365 * 5)
        
        with engine.connect() as conn:
            query = text("""
                SELECT kb.id, kb.ticker, kb.ts, kb.event_type, kb.content
                FROM knowledge_base kb
                INNER JOIN (SELECT DISTINCT ticker FROM quotes) q ON q.ticker = kb.ticker
                WHERE kb.ts <= :cutoff_date
                  AND kb.ts >= :min_date
                  AND kb.ticker IS NOT NULL
                  AND kb.content IS NOT NULL
                  AND LENGTH(TRIM(kb.content)) > 10
                  AND (kb.outcome_json IS NULL OR kb.outcome_json::text = 'null'::text)
                ORDER BY kb.ts DESC
                LIMIT :lim
            """)
            
            params = {
                "cutoff_date": cutoff_date,
                "min_date": min_date,
                "lim": limit
            }
            
            events_df = pd.read_sql(query, conn, params=params)
            
            if events_df.empty:
                logger.info("‚ÑπÔ∏è –ù–µ—Ç —Å–æ–±—ã—Ç–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏—Å—Ö–æ–¥–æ–≤ (—Ç–æ–ª—å–∫–æ —Ç–∏–∫–µ—Ä—ã –∏–∑ quotes, —Å–æ–±—ã—Ç–∏—è –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5 –ª–µ—Ç, —Å—Ç–∞—Ä—à–µ %s –¥–Ω.)", days_after)
                return
            
            logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(events_df)} —Å–æ–±—ã—Ç–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏—Å—Ö–æ–¥–æ–≤ (—Ç–∏–∫–µ—Ä—ã –µ—Å—Ç—å –≤ quotes, —Å–æ–±—ã—Ç–∏—è —Å—Ç–∞—Ä—à–µ {days_after} –¥–Ω., –Ω–µ —Å—Ç–∞—Ä—à–µ 5 –ª–µ—Ç)")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á–∞–º–∏
            for i in range(0, len(events_df), batch_size):
                batch = events_df.iloc[i:i+batch_size]
                
                for _, row in batch.iterrows():
                    try:
                        event_id = int(row['id'])
                        ticker = row['ticker']
                        event_ts = row['ts']
                        
                        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏—Å—Ö–æ–¥ —Å–æ–±—ã—Ç–∏—è
                        outcome = analyzer.analyze_event_outcome(
                            event_id=event_id,
                            ticker=ticker,
                            days_after=days_after,
                            event_ts=event_ts
                        )
                        
                        if outcome:
                            # –û–±–Ω–æ–≤–ª—è–µ–º outcome_json
                            success = analyzer.update_event_outcome(event_id, outcome)
                            if success:
                                updated_count += 1
                                logger.debug(
                                    f"‚úÖ –°–æ–±—ã—Ç–∏–µ ID={event_id} ({ticker}): "
                                    f"–∏–∑–º–µ–Ω–µ–Ω–∏–µ {outcome.get('price_change_pct', 0):.2f}%, "
                                    f"–∏—Å—Ö–æ–¥ {outcome.get('outcome', 'UNKNOWN')}"
                                )
                            else:
                                error_count += 1
                        else:
                            skipped_count += 1
                            logger.debug(
                                f"‚ö†Ô∏è –°–æ–±—ã—Ç–∏–µ ID={event_id} ({ticker}): "
                                f"–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ –∫–æ—Ç–∏—Ä–æ–≤–∫–∞—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"
                            )
                        
                        analyzed_count += 1
                        
                    except Exception as e:
                        error_count += 1
                        logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Å–æ–±—ã—Ç–∏—è ID={row['id']}: {e}")
                
                logger.info(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {min(i+batch_size, len(events_df))}/{len(events_df)} —Å–æ–±—ã—Ç–∏–π")
        
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∏—Å—Ö–æ–¥–æ–≤: {e}", exc_info=True)
    
    logger.info("=" * 60)
    logger.info(
        f"‚úÖ –ê–Ω–∞–ª–∏–∑ –∏—Å—Ö–æ–¥–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω: "
        f"–ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {analyzed_count}, "
        f"–æ–±–Ω–æ–≤–ª–µ–Ω–æ {updated_count}, "
        f"–ø—Ä–æ–ø—É—â–µ–Ω–æ {skipped_count}, "
        f"–æ—à–∏–±–æ–∫ {error_count}"
    )
    logger.info("=" * 60)


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
    days_after = int(os.getenv('EVENT_OUTCOME_DAYS_AFTER', '7'))
    limit = None
    if os.getenv('EVENT_OUTCOME_LIMIT'):
        try:
            limit = int(os.getenv('EVENT_OUTCOME_LIMIT'))
        except ValueError:
            logger.warning(f"‚ö†Ô∏è –ù–µ–≤–µ—Ä–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ EVENT_OUTCOME_LIMIT: {os.getenv('EVENT_OUTCOME_LIMIT')}")
    
    batch_size = int(os.getenv('EVENT_OUTCOME_BATCH_SIZE', '50'))
    
    analyze_existing_events(
        days_after=days_after,
        limit=limit,
        batch_size=batch_size
    )


if __name__ == "__main__":
    main()
